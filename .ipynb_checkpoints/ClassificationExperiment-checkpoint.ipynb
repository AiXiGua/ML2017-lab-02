{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path_train = './data/a9a'\n",
    "path_validation = './data/a9a.t'\n",
    "\n",
    "def load_data(file_path):  \n",
    "#     读取libsvm格式数据  \n",
    "    x, y = load_svmlight_file(file_path)  \n",
    "    return x, y\n",
    "\n",
    "def pre_process():\n",
    "#     读取训练数据\n",
    "    X_train, y_train = load_data(path_train)\n",
    "    X_train = X_train.toarray()\n",
    "#     读取训练数据\n",
    "    X_validation, y_validation = load_data(path_validation)\n",
    "    X_validation = X_validation.toarray()\n",
    "#     补全稀疏矩阵\n",
    "    column = np.zeros(( X_validation.shape[0]))\n",
    "    X_validation = np.column_stack((X_validation, column))\n",
    "    #y = W^T *X + b -> y = W_extend^T * [X,1]\n",
    "    column_train = np.ones(( X_train.shape[0]))\n",
    "    column_validation = np.ones((X_validation.shape[0]))\n",
    "    X_train = np.column_stack((X_train, column_train))\n",
    "    X_validation = np.column_stack((X_validation, column_validation))    \n",
    "    return X_train, y_train, X_validation, y_validation\n",
    "\n",
    "def get_initial_parameter(X_train):\n",
    "    m, n = X_train.shape\n",
    "    initial_theta = np.zeros(n)\n",
    "    v_t = np.zeros(n)\n",
    "    return initial_theta, v_t\n",
    "    \n",
    "#draw the result\n",
    "def draw_plot(Loss_train, Loss_validation, name):\n",
    "    plt.plot(Loss_train, label=\"Loss_train\")\n",
    "    plt.plot(Loss_validation, label=\"Loss_validation\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Logistic regression optimized by \" + name)\n",
    "    plt.show()\n",
    "\n",
    "#shuffles the array\n",
    "def shuffle_array(X_train, y_train):\n",
    "    randomlist = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(randomlist)\n",
    "    X_random = X_train[randomlist]\n",
    "    y_random = y_train[randomlist]\n",
    "    return X_random, y_random\n",
    "\n",
    "#get the training instance and label in current batch\n",
    "def get_batch(i, iters, X_random, y_random, batch_size, shape):\n",
    "    if i == iters - 1:\n",
    "        X_batch = X_random[i * batch_size: shape + 1]\n",
    "        y_batch = y_random[i * batch_size: shape + 1]\n",
    "    else:\n",
    "        X_batch = X_random[i * batch_size: (i + 1) * batch_size]\n",
    "        y_batch = y_random[i * batch_size: (i + 1) * batch_size]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "#calculate the loss\n",
    "def compute_loss(X, y, theta, lamda):\n",
    "    m = y.shape[0]\n",
    "    C = np.maximum(np.ones(m) - X.dot(theta) * y, np.zeros(m))\n",
    "    loss = np.sum(C) / X.shape[0] + (lamda / 2) * LA.norm(theta, 2) ** 2 \n",
    "    return loss\n",
    "\n",
    "#calculate the gradient\n",
    "def compute_gradient(X, y, theta, lamda):\n",
    "    Xy = X * y.reshape((-1,1)) \n",
    "    G = - Xy.T.dot(Xy.dot(theta) <= 1)\n",
    "    G = G / X.shape[0] + theta * lamda\n",
    "    return G   \n",
    "\n",
    "def NAG():\n",
    "#     Hyper parameters\n",
    "    eta = 0.001\n",
    "    epoch = 5\n",
    "    gamma = 0.9\n",
    "    lamda = 0.01\n",
    "    batch_size = 128\n",
    "    \n",
    "#     get initial data\n",
    "    X_train, y_train, X_validation, y_validation = pre_process()\n",
    "    theta, v_t = get_initial_parameter(X_train)\n",
    "    num_iter = math.ceil(X_train.shape[0] / float(batch_size))\n",
    "    sum_iteration = epoch * num_iter\n",
    "    Loss_train_his = np.zeros(sum_iteration)\n",
    "    Loss_validation_his = np.zeros(sum_iteration)\n",
    "\n",
    "#     optimize the parameter theta\n",
    "    for i in range(0, epoch):\n",
    "        X_random,y_random = shuffle_array(X_train, y_train)\n",
    "        for iter in range(0, num_iter):\n",
    "            #get the training instance and label in current batch\n",
    "            X_batch, y_batch = get_batch(i, num_iter, X_random, y_random, batch_size, X_train.shape[0])\n",
    "            #approximate theta in the next time step\n",
    "            theta_t = theta - v_t * gamma\n",
    "            #the training loss\n",
    "            Loss_train_his[i * num_iter + iter] = compute_loss(X_batch, y_batch, theta, lamda)\n",
    "            #the gradient of the loss function\n",
    "            G = compute_gradient(X_batch, y_batch, theta_t, lamda)\n",
    "            #the validation loss\n",
    "            Loss_validation_his[i * num_iter + iter] = compute_loss(X_validation, y_validation, theta, lamda)\n",
    "            #update the parameter theta and momentum\n",
    "            v_t = v_t * gamma + G * eta\n",
    "            theta = theta - v_t\n",
    "    #draw the result\n",
    "    draw_plot(Loss_train_his, Loss_validation_his, 'NAG')\n",
    "\n",
    "def RMSprop():\n",
    "#     Hyper parameters\n",
    "    eta = 0.001\n",
    "    epoch = 5\n",
    "    lamda = 0.01\n",
    "    gamma = 0.9\n",
    "    batch_size = 128\n",
    "    epsilon = np.e ** (- 8)\n",
    "\n",
    "#     get initial data\n",
    "    X_train, y_train, X_validation, y_validation = pre_process()\n",
    "    theta, v_t = get_initial_parameter(X_train)\n",
    "    num_iter = math.ceil(X_train.shape[0] / float(batch_size))\n",
    "    sum_iteration = epoch * num_iter\n",
    "    Loss_train_his = np.zeros(sum_iteration)\n",
    "    Loss_validation_his = np.zeros(sum_iteration)\n",
    "    G_square = 0\n",
    "\n",
    "#     optimize the parameter theta\n",
    "    for i in range(0, epoch):\n",
    "        X_random,y_random = shuffle_array(X_train, y_train)\n",
    "        for iter in range(0, num_iter):\n",
    "            #get the training instance and label in current batch\n",
    "            X_batch, y_batch = get_batch(i, num_iter, X_random, y_random, batch_size, X_train.shape[0])\n",
    "            #the training loss\n",
    "            Loss_train_his[i * num_iter + iter] = compute_loss(X_batch, y_batch, theta, lamda)\n",
    "            #the gradient of the loss function\n",
    "            G = compute_gradient(X_batch, y_batch, theta, lamda)\n",
    "            #the validation loss\n",
    "            Loss_validation_his[i * num_iter + iter] = compute_loss(X_validation, y_validation, theta, lamda)\n",
    "            #update the parameter theta and the square of the gradient\n",
    "            G_square = G_square * gamma + np.dot(G,G.T) * (1 - gamma)\n",
    "            theta = theta - G * (eta / math.sqrt(G_square + epsilon))\n",
    "    #draw the result\n",
    "    draw_plot(Loss_train_his, Loss_validation_his, 'RMSprop')\n",
    "\n",
    "def AdaDelta():\n",
    "#     Hyper parameters\n",
    "    eta = 0.001\n",
    "    epoch = 5\n",
    "    lamda = 0.01\n",
    "    gamma = 0.9\n",
    "    batch_size = 3000\n",
    "    epsilon = np.e ** (- 8)\n",
    "    delta_t = 0.01\n",
    "\n",
    "#     get initial data\n",
    "    X_train, y_train, X_validation, y_validation = pre_process()\n",
    "    theta, v_t = get_initial_parameter(X_train)\n",
    "    num_iter = math.ceil(X_train.shape[0] / float(batch_size))\n",
    "    sum_iteration = epoch * num_iter\n",
    "    Loss_train_his = np.zeros(sum_iteration)\n",
    "    Loss_validation_his = np.zeros(sum_iteration)\n",
    "    G_square = 0\n",
    "\n",
    "#     optimize the parameter theta\n",
    "    for i in range(0, epoch):\n",
    "        X_random,y_random = shuffle_array(X_train, y_train)\n",
    "        for iter in range(0, num_iter):\n",
    "            #get the training instance and label in current batch\n",
    "            X_batch, y_batch = get_batch(i, num_iter, X_random, y_random, batch_size, X_train.shape[0])\n",
    "            #the training loss\n",
    "            Loss_train_his[i * num_iter + iter] = compute_loss(X_batch, y_batch, theta, lamda)\n",
    "            #the gradient of the loss function\n",
    "            G = compute_gradient(X_batch, y_batch, theta, lamda)\n",
    "            #the validation loss\n",
    "            Loss_validation_his[i * num_iter + iter] = compute_loss(X_validation, y_validation, theta, lamda)\n",
    "            #update the parameter theta and the square of the gradient\n",
    "            G_square = G_square * gamma + np.dot(G,G.T) * (1 - gamma)\n",
    "            delta_theta = - G * ((delta_t + epsilon) ** 0.5 / (G_square + epsilon) ** 0.5)\n",
    "            theta += delta_theta\n",
    "            delta_t = delta_t * gamma + (1 - gamma) * delta_theta **2 \n",
    "    #draw the result\n",
    "    draw_plot(Loss_train_his, Loss_validation_his, 'AdaDelta')\n",
    "\n",
    "def Adam():\n",
    "#     Hyper parameters\n",
    "    eta = 0.001\n",
    "    epoch = 1\n",
    "    lamda = 0.01\n",
    "    gamma = 0.999\n",
    "    batch_size = 16\n",
    "    epsilon = np.e ** (- 8)\n",
    "    beta = 0.9\n",
    "\n",
    "#     get initial data\n",
    "    X_train, y_train, X_validation, y_validation = pre_process()\n",
    "    theta, v_t = get_initial_parameter(X_train)\n",
    "    num_iter = math.ceil(X_train.shape[0] / float(batch_size))\n",
    "    sum_iteration = epoch * num_iter\n",
    "    Loss_train_his = np.zeros(sum_iteration)\n",
    "    Loss_validation_his = np.zeros(sum_iteration)\n",
    "    G_square = 0\n",
    "    m_t = 0\n",
    "    alpha = 0\n",
    "\n",
    "#     optimize the parameter theta\n",
    "    for i in range(0, epoch):\n",
    "        X_random,y_random = shuffle_array(X_train, y_train)\n",
    "        for iter in range(0, num_iter):\n",
    "            #get the training instance and label in current batch\n",
    "            X_batch, y_batch = get_batch(i, num_iter, X_random, y_random, batch_size, X_train.shape[0])\n",
    "            #the training loss\n",
    "            Loss_train_his[i * num_iter + iter] = compute_loss(X_batch, y_batch, theta, lamda)\n",
    "            #the gradient of the loss function\n",
    "            G = compute_gradient(X_batch, y_batch, theta, lamda)\n",
    "            #the validation loss\n",
    "            Loss_validation_his[i * num_iter + iter] = compute_loss(X_validation, y_validation, theta, lamda)\n",
    "            #update the parameter theta and the square of the gradient\n",
    "            m_t = beta * m_t + (1 - beta) * G\n",
    "            G_square = G_square * gamma + np.dot(G,G.T) * (1 - gamma)\n",
    "            alpha = eta * ((1 - gamma ** (iter + 1)) / (1 - beta ** (iter + 1)))\n",
    "            theta = theta - alpha * (m_t / (G_square + epsilon) ** 0.5)\n",
    "    #draw the result\n",
    "    draw_plot(Loss_train_his, Loss_validation_his, 'Adam')\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     NAG()\n",
    "#     RMSprop()\n",
    "#     AdaDelta()\n",
    "#     Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:96: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:97: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "range() integer end argument expected, got float.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e5c80a6ef1a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNAG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mAdaDelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-4d27fb384433>\u001b[0m in \u001b[0;36mNAG\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mX_random\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;31m#get the training instance and label in current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_random\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: range() integer end argument expected, got float."
     ]
    }
   ],
   "source": [
    "NAG()\n",
    "RMSprop()\n",
    "AdaDelta()\n",
    "Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

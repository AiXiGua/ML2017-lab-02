{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "path_train = './data/a9a'\n",
    "path_validation = './data/a9a.t'\n",
    "\n",
    "def load_data(file_path):  \n",
    "#     读取libsvm格式数据  \n",
    "    x, y = load_svmlight_file(file_path)  \n",
    "    return x, y\n",
    "\n",
    "def pre_process():\n",
    "#     读取训练数据\n",
    "    X_train, y_train = load_data(path_train)\n",
    "    X_train = X_train.toarray()\n",
    "#     读取训练数据\n",
    "    X_validation, y_validation = load_data(path_validation)\n",
    "    X_validation = X_validation.toarray()\n",
    "#     补全稀疏矩阵\n",
    "#     column = np.zeros(( X_validation.shape[0]))\n",
    "#     X_validation = np.column_stack((X_validation,column))\n",
    "    #y = W^T *X + b -> y = W_extend^T * [X,1]\n",
    "    column_train = np.ones(( X_train.shape[0]))\n",
    "    column_validation = np.ones(( X_validation.shape[0]))\n",
    "    X_train = np.column_stack((X_train,column_train))\n",
    "    X_validation = np.column_stack((X_validation,column_validation))    \n",
    "    return X_train, y_train, X_validation, y_validation\n",
    "\n",
    "def get_initial_parameter(X_train):\n",
    "    m, n = X_train.shape[1]\n",
    "    initial_theta = np.zeros(n)\n",
    "    v_t = np.zeros(n)\n",
    "    return initial_theta, v_t\n",
    "    \n",
    "#draw the result\n",
    "def draw_plot(Loss_train, Loss_validation, name):\n",
    "    plt.plot(Loss_train, label=\"Loss_train\")\n",
    "    plt.plot(Loss_validation, label=\"Loss_validation\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Logistic regression optimized by \" + name)\n",
    "    plt.show()\n",
    "\n",
    "#shuffles the array\n",
    "def shuffle_array(X_train):\n",
    "    randomlist = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(randomlist)\n",
    "    X_random = X_train[randomlist]\n",
    "    y_random = y_train[randomlist]\n",
    "    return X_random, y_random\n",
    "\n",
    "#get the training instance and label in current batch\n",
    "def get_batch(iters, X_random, y_random, batch_size, shape):\n",
    "    if l == iters - 1:\n",
    "        X_batch = X_random[l * batch_size: shape + 1]\n",
    "        y_batch = y_random[l * batch_size: shape + 1]\n",
    "    else:\n",
    "        X_batch = X_random[l * batch_size: (l + 1) * batch_size]\n",
    "        y_batch = y_random[l * batch_size: (l + 1) * batch_size]\n",
    "    return X_batch, y_batch\n",
    "\n",
    "#calculate the loss\n",
    "def compute_loss(X, theta, y, lamda):\n",
    "    preY = np.dot(X, theta)\n",
    "    Loss = np.sum(np.log(1 + np.exp(- y * preY)))\n",
    "    Loss = Loss / X.shape[0] + lamda / 2 * np.dot(theta, theta.T)\n",
    "    return Loss\n",
    "\n",
    "#calculate the gradient\n",
    "def compute_gradient(X, theta, y, lamda):\n",
    "    preY = np.dot(X, theta)\n",
    "    G = np.dot(((- y) / (1 + np.exp(y * preY))), X )\n",
    "    G = G / X.shape[0] + theta * lamda\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 124)\n",
      "(32561,)\n",
      "(16281, 124)\n",
      "(16281,)\n"
     ]
    }
   ],
   "source": [
    "pre_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use different optmization to update the parameter $\\theta$\n",
    "\n",
    "-  Nesterov accelerated gradient(NAG)\n",
    "$$g_t\\leftarrow\\triangledown  _{\\theta}J(\\theta -\\gamma  \\: v_{t-1})$$\n",
    "$$v_t \\leftarrow \\gamma \\: v_{t-1}+\\eta g_t$$\n",
    "$$\\theta\\leftarrow \\theta - v_t$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NAG():\n",
    "    lr = 0.02\n",
    "    epoch = 5\n",
    "    gamma = 0.9\n",
    "    lamda = 0.01\n",
    "    batch_size = 128\n",
    "    num_iter = math.ceil(X_train.shape[0] / float(batch_size))\n",
    "    sum_iteration = epoch * num_iter\n",
    "#     get initial data\n",
    "    X_train, y_train, X_validation, y_validation, initial_theta = pre_process()\n",
    "    theta, v_t = get_initial_parameter(X_train)\n",
    "    Loss_train_his = np.zeros(sum_iteration)\n",
    "    Loss_validation_his = np.zeros(sum_iteration)\n",
    "    \n",
    "#     optimize the parameter theta\n",
    "    for i in range(0, epoch):\n",
    "        X_random,y_random = shuffle_array(X_train)\n",
    "        for iter in range(0, num_iter):\n",
    "            #get the training instance and label in current batch\n",
    "            X_batch, y_batch = get_Batch(num_iter, X_random, y_random, batch_size, X_train.shape[0])\n",
    "            #approximate theta in the next time step\n",
    "            theta_t = theta - v_t * gamma\n",
    "            #the training loss\n",
    "            Loss_train_his[i * num_iter + iter] = compute_loss(X_batch, theta, y_batch, lamda)\n",
    "            #the gradient of the loss function\n",
    "            G = compute_gradient(X_batch, theta_t, y_batch, lamda)\n",
    "            #the validation loss\n",
    "            Loss_validation_his[i * num_iter + iter] = compute_loss(X_validation, theta, y_validation, lamda)\n",
    "            #update the parameter W\n",
    "            v_t = v_t * gamma + G * lr\n",
    "            theta = theta - v_t\n",
    "    #draw the result\n",
    "    draw_plot(Loss_train, Loss_validation, 'NAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
